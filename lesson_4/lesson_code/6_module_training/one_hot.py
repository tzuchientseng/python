# one hot (獨熱)
import numpy as np
a = np.zeros([10, 5])
print(a)
# 早期認為 one hot 可以加快且方便演算，但目前已慢慢放棄掉了
# [1. 0. 0. 0. 0.] : 雛菊
# [0. 1. 0. 0. 0.] : 蒲公英
# [0. 0. 1. 0. 0.] : 玫瑰
# [0. 0. 0. 1. 0.] : 向日葵
# [0. 0. 0. 0. 1.] : 鬱金香

"""
One-hot 編碼曾經被廣泛使用，但由於模型架構的一些限制和進步，在許多機器學習和深度學習任務中逐漸被其他技術取代或補充。原因如下：

1.大特徵空間中的高維
問題：One-hot 編碼會建立一個大型稀疏矩陣，特別是在處理具有高基數的類別（例如，數千個唯一類別）時。
影響：這會增加記憶體使用量和計算開銷，這對於模型和硬體來說可能效率低。
2.缺乏語意訊息
問題：One-hot 編碼將所有類別視為彼此等距，從而忽略了類別之間的任何潛在語義關係。
例如：在植物分類任務中，「玫瑰」和「鬱金香」可能比「玫瑰」和「向日葵」更相似，但 one-hot 編碼並不能反映這一點。
3.大輸出的可擴展性差
問題：在自然語言處理或電腦視覺等任務中，輸出空間可能會變得非常大（例如，詞彙表中有數萬個單字）。
影響：one-hot 向量的稀疏性質使其對於訓練現代大規模模型來說不切實際。
4.表徵學習的進展
替代方案：
嵌入層：密集向量表示（嵌入）捕捉低維空間中的語意資訊和類別之間的關係。 Word2Vec、GloVe 或 fastText 等模型使這一切成為可能。
預訓練模型：對於 NLP，BERT、GPT 或 T5 等轉換器可以直接提供更豐富的上下文嵌入，從而無需手動進行 one-hot 編碼。
5.與神經網路架構的兼容性
問題：神經網絡，尤其是深層神經網絡，很難處理諸如 one-hot 編碼之類的稀疏表示，因為它們不能有效地利用稀疏性。
改進：密集嵌入（學習或預訓練）無縫整合到神經網路中，使訓練更快、更有效。
6.效能瓶頸
深度學習範例：
One-hot 編碼需要矩陣運算，可能涉及乘以大型稀疏矩陣。雖然優化的函式庫可以處理這個問題，但密集嵌入本質上運算效率更高。
One-Hot 編碼仍然可行的用例
當類別數量較少時（例如，少於 10-20 個類別）。
對於線性迴歸、決策樹等簡單模型，或當可解釋性是優先考慮的時候。
在分類關係不相關或不會顯著影響模型表現的問題。
總之，雖然 one-hot 編碼是一種簡單直觀的方法，但其效率低下以及更複雜方法的興起使其在現代機器學習工作流程中不太受歡迎。
"""
